{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":420,"sourceType":"datasetVersion","datasetId":19}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Machine Learning Guide With Optuna**","metadata":{}},{"cell_type":"code","source":"# Import \nimport pandas as pd \nimport numpy as np \nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport optuna\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import *\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:44:11.643486Z","iopub.execute_input":"2024-06-30T18:44:11.643816Z","iopub.status.idle":"2024-06-30T18:44:12.847740Z","shell.execute_reply.started":"2024-06-30T18:44:11.643785Z","shell.execute_reply":"2024-06-30T18:44:12.846305Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Some Basic Setting\nN_SPLIT = 10\nSEED = 42","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:44:12.848946Z","iopub.execute_input":"2024-06-30T18:44:12.849421Z","iopub.status.idle":"2024-06-30T18:44:12.854202Z","shell.execute_reply.started":"2024-06-30T18:44:12.849388Z","shell.execute_reply":"2024-06-30T18:44:12.853030Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"# Load Data \ndf = pd.read_csv('/kaggle/input/iris/Iris.csv')\n\n# Head \nprint(df.head())\n\n# Target Count \nprint('----------------------------------------------')\nprint(df['Species'].value_counts())\nprint('----------------------------------------------')\n\n# Encode The Target as in PReprocessing\n\n# Label Encoder ko Intlize krien\nLE = LabelEncoder()\n\n# Es ko Fit krien \ndf['Species'] = LE.fit_transform(df['Species'])\nprint('Target Encoded ')","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:44:12.855427Z","iopub.execute_input":"2024-06-30T18:44:12.856140Z","iopub.status.idle":"2024-06-30T18:44:12.894219Z","shell.execute_reply.started":"2024-06-30T18:44:12.856110Z","shell.execute_reply":"2024-06-30T18:44:12.892897Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n0   1            5.1           3.5            1.4           0.2  Iris-setosa\n1   2            4.9           3.0            1.4           0.2  Iris-setosa\n2   3            4.7           3.2            1.3           0.2  Iris-setosa\n3   4            4.6           3.1            1.5           0.2  Iris-setosa\n4   5            5.0           3.6            1.4           0.2  Iris-setosa\n----------------------------------------------\nSpecies\nIris-setosa        50\nIris-versicolor    50\nIris-virginica     50\nName: count, dtype: int64\n----------------------------------------------\nTarget Encoded \n","output_type":"stream"}]},{"cell_type":"code","source":"# Tips \ntip = sns.load_dataset('tips')\n\nprint(tip.head())\n\nprint('----------------------------------------------')\nprint(tip['sex'].value_counts())\nprint('----------------------------------------------')\n\n# Encode The Target as in PReprocessing\n# Label Encoder ko Intlize krien\nLE = LabelEncoder()\n\n# Es ko Fit krien \ntip['sex'] = LE.fit_transform(tip['sex'])\ntip['smoker'] = LE.fit_transform(tip['smoker'])\ntip['day'] = LE.fit_transform(tip['day'])\ntip['time'] = LE.fit_transform(tip['time'])\n\nprint('Data Encoded ')\nprint('----------------------------------------------')\n\n# Scale Krien Data ko \n# Scaler Ko Intilize krn \nsc = StandardScaler()\n\ntip[['tip','total_bill']] = sc.fit_transform(tip[['tip','total_bill']])\nprint('Data Scaled ')\nprint('----------------------------------------------')\n\nprint(tip.head())","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:44:12.897791Z","iopub.execute_input":"2024-06-30T18:44:12.898219Z","iopub.status.idle":"2024-06-30T18:44:12.928954Z","shell.execute_reply.started":"2024-06-30T18:44:12.898188Z","shell.execute_reply":"2024-06-30T18:44:12.927619Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"   total_bill   tip     sex smoker  day    time  size\n0       16.99  1.01  Female     No  Sun  Dinner     2\n1       10.34  1.66    Male     No  Sun  Dinner     3\n2       21.01  3.50    Male     No  Sun  Dinner     3\n3       23.68  3.31    Male     No  Sun  Dinner     2\n4       24.59  3.61  Female     No  Sun  Dinner     4\n----------------------------------------------\nsex\nMale      157\nFemale     87\nName: count, dtype: int64\n----------------------------------------------\nData Encoded \n----------------------------------------------\nData Scaled \n----------------------------------------------\n   total_bill       tip  sex  smoker  day  time  size\n0   -0.314711 -1.439947    0       0    2     0     2\n1   -1.063235 -0.969205    1       0    2     0     3\n2    0.137780  0.363356    1       0    2     0     3\n3    0.438315  0.225754    1       0    2     0     2\n4    0.540745  0.443020    0       0    2     0     4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# <p style=\"background-color: #00A4CCFF; font-family:Pacifico; color:#E6E6FA; font-size:200%; text-align:center; border-radius:0%;\">Machine Learning</p>\n\nI Will Show you How To Train Model and , Which Things Effect the Model , How to Get Accurate Model Each and Everthing Will Be Discussed Here.","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color: #00A4CCFF; font-family: Pacifico, cursive; color: #E6E6FA; font-size: 100%; text-align: left; border-radius: 5px; padding: 10px 30px; display: inline-block;\">Simple Training</p>","metadata":{}},{"cell_type":"code","source":"# Sab se phly ham libraries ko import kr lyn gye Jo ke Important hain Hamaray Liye , Meny Idhr Data Bilkul Small Liye hy Maybe Idhr Apko\n# Optuna ke Results me itna Farak Show na ho Liken Optuna is Best For Large Datsets.\n# Me idhr bs 4 Models ko hi Import krn ga ap Apni Problem ke Hisab se Mukhtalif Models ko Use kr Sktay Hain \n\n# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\n# CatBoost\nfrom catboost import CatBoostClassifier\n\n# LightGBM\nfrom lightgbm import LGBMClassifier\n\n# XGBoost\nfrom xgboost import XGBClassifier\n\n# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import The Metrix \nfrom sklearn.metrics import *\n\n\n# # X and Y ko Alg Kr Lyn Ek br \nX_iris = df.drop('Species',axis=1)\ny_iris = df['Species']\n\n# Simple Train Test Split \nX_train_iris,X_test_iris,y_train_iris,y_test_iris = train_test_split(X_iris,y_iris,test_size=0.2,random_state=42)\n\n# # X and Y ko Alg Kr Lyn Ek br \nX_tip = tip.drop('sex',axis=1)\ny_tip = tip['sex']\n\n# Simple Train Test Split \nX_train_tip,X_test_tip,y_train_tip,y_test_tip = train_test_split(X_tip,y_tip,test_size=0.2,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:44:25.320418Z","iopub.execute_input":"2024-06-30T18:44:25.320817Z","iopub.status.idle":"2024-06-30T18:44:25.336153Z","shell.execute_reply.started":"2024-06-30T18:44:25.320788Z","shell.execute_reply":"2024-06-30T18:44:25.334816Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"background-color: #00A4CCFF; font-family: Pacifico, cursive; color: #E6E6FA; font-size: 100%; text-align: left; border-radius: 5px; padding: 10px 30px; display: inline-block;\">1. Iris</p>","metadata":{}},{"cell_type":"code","source":"# Acha to Ab hamaray models import ho gye hain ab ham sb se phly , in models me se best model ko select krien gyen\n# Jo Model sb se acha ho ga us pa ham further Parameter tunning krien gye.\n\n# Best Model ko Kesy Find Kia Jaa Skta hy , Ager Data Set Bht Large ho to tb kesy Best Model ko Nikalna hy , Chalien Sb Se Phly Dekhtay\n# Hain How to Get Best Model\ndef Check_All(X_train,X_test,y_train,y_test):\n    # Sb se Phly Ham ek Dic Bana Lyn Gye Models ki With Their names.\n    models = {\n        'RF' : RandomForestClassifier(random_state=SEED),\n        'DT' : DecisionTreeClassifier(random_state=SEED),\n        'XGB': XGBClassifier(random_state=SEED),\n        'CAT': CatBoostClassifier(verbose=0,random_state=SEED), # Idhr me Verbose 0 or -1 is liye pass kr rha hn ku ke Output ko Truncate kr rha \n        'LGBM': LGBMClassifier(verbose=-1,random_state=SEED) # hn CatBoost aur LGBM ki \n    }\n\n    # Ab Hamara Next Step ho ga Ham ek Loop Run Krien Gye Jo ke Hamarai Dic me se Model aur un ke Names ko Pick Kry ga aur Phr One \n    # by one Un Models ko Train kr Ky Un ke Resultsn ko Store kry ga Then ham un Results ko ek Dataframe me Store Krwaien Gye. \n\n    # Idhr ab ek List bhi Intilize krien gye Jis me Hamary Scores aur Model ka Name Store Hota Rahe \n    Scores_of_Models = []\n\n    # Lets Run The Loop.\n    for name,model in models.items():  # .items() use karna zaroori hai\n        M = model\n\n        # Train Model \n        M.fit(X_train,y_train)\n\n        # Predict \n        y_Pred = M.predict(X_test)\n\n        # Idhr me Bs Accuracy is Check krn ga , Ap Apni Problem ke Hisab se Kr Sktay Hain Kuch aur Matrix add\n        acc = accuracy_score(y_test,y_Pred)\n\n        Scores_of_Models.append((name,acc))\n\n        # Print The Score \n    #     print(f'Accuracy of The {name} is {acc}')\n\n    # Ab Jo Scores Ayen Hain Ham Un Ko Ek Data Frame me Ly Jayen gye aur Show Krwaien Gye \n    scores_df = pd.DataFrame(Scores_of_Models, columns=['Model', 'Accuracy'])\n    return scores_df\n\nCheck_All(X_train_iris,X_test_iris,y_train_iris,y_test_iris)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:44:25.905520Z","iopub.execute_input":"2024-06-30T18:44:25.905918Z","iopub.status.idle":"2024-06-30T18:44:27.181909Z","shell.execute_reply.started":"2024-06-30T18:44:25.905887Z","shell.execute_reply":"2024-06-30T18:44:27.180712Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"  Model  Accuracy\n0    RF       1.0\n1    DT       1.0\n2   XGB       1.0\n3   CAT       1.0\n4  LGBM       1.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>RF</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DT</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>XGB</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CAT</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LGBM</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"- **train_test_split ko use krty huway hamary sab model ne 100% acc achieve kii hy , dekhnay me to yeh bht Acha lag rha hy , now I have a Question Kia Yeh Sb models Accurate hain? Kiya inhon ne jo score dia hy woh asal me bhi itna hi hy? Kiye yeh model 100% sahi jawab hi deyn gyen ? Ab In Question Ko Ly ke Chaltay hain aur me explain krn ga ke yeh results sahi nahi , hamien es pa focus krna ho ga aur inhien acha krna ho ga.**\n\n- Ab ager Ek Non-Expereience band Yeh Result Dekhay Ga to Woh to Inhie ko Best Score man ly gye aur in Future us ka Model sahi predictions nahi dey ga.","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color: #00A4CCFF; font-family: Pacifico, cursive; color: #E6E6FA; font-size: 100%; text-align: left; border-radius: 5px; padding: 10px 30px; display: inline-block;\">2. Tip</p>","metadata":{}},{"cell_type":"code","source":"Check_All(X_train_tip,X_test_tip,y_train_tip,y_test_tip)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:44:28.389775Z","iopub.execute_input":"2024-06-30T18:44:28.390175Z","iopub.status.idle":"2024-06-30T18:44:29.603561Z","shell.execute_reply.started":"2024-06-30T18:44:28.390145Z","shell.execute_reply":"2024-06-30T18:44:29.602323Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"  Model  Accuracy\n0    RF  0.591837\n1    DT  0.591837\n2   XGB  0.632653\n3   CAT  0.612245\n4  LGBM  0.673469","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>RF</td>\n      <td>0.591837</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DT</td>\n      <td>0.591837</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>XGB</td>\n      <td>0.632653</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CAT</td>\n      <td>0.612245</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LGBM</td>\n      <td>0.673469</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# <p style=\"background-color: #00A4CCFF; font-family: Pacifico, cursive; color: #E6E6FA; font-size: 100%; text-align: left; border-radius: 5px; padding: 10px 30px; display: inline-block;\">More Effiecient Trainig</p>","metadata":{}},{"cell_type":"markdown","source":"# Cross Validation Split\n\nModel ko train krny ke 2 Tareeqay hain ek to Simple tareeqa hy ke ham direct model ko Train kr deyn... Without any Cross Validation, matlb Model ko Cross validate kiya hi na Jaye woh jo ek br me Results Show kr rha hy usi pa yaqeen kr lia jaye..\n\nek tareeeqa hota hy ke model ko cross_validate kia Jaye... Cross_validation ek achi approch hoti hy es se hamay accurate and best models miltay hain. \n\nAb Main Sawal Yeh hy ke Kia Hamay Hr Bar hi Cross_Validation Krni Chaiye ? Ya Kuch Special case hain Jahan Es ko use krna Bht Zarori Smjha Jata hy..\n\nChalo Ab Yeh Smjhtay Hain ke Cross_Validation kia hy aur es ka use Ku Zarori hy.\n\n# What is Cross Validation / CV\nCross-validation (CV) ek technique hai jo machine learning model ki performance ko evaluate aur test karne ke liye istemal hoti hai. Ye technique aam tor par applied ML tasks mein istemal hoti hai. Iska maqsad kisi specific predictive modeling problem ke liye behtareen model ko compare aur select karna hota hai.\n\nCV samajhne mein asaan hai, implement karna asaan hai, aur yeh doosri methods se kam bias rakhti hai jo model ki efficiency ko count karne mein istemal hoti hain. Yeh sab qualities cross-validation ko specific tasks ke liye behtareen model select karne ke liye ek taqatwar tool banati hain.\n\n# Kab Cross Validation ko use krna Chahiye \nAger to Short Awnser chahiye to CV ko tb use krna chahiye jab data kam ho apke pass.\n\nCross-validation ko tab use karna chahiye jab aap machine learning model ki performance ko sahi tareeqe se evaluate karna chahte hain aur model ko robust banane ki zaroorat hai. Jab aap ek predictive modeling problem ke liye behtareen model select karna chahte hain, to cross-validation ek zaroori tool hai.\n\nIs technique ka istemal tab kiya jata hai jab aapko model ki accuracy, precision, recall, aur doosre performance metrics ko sahi tareeqe se measure karna hai. Cross-validation models ke bias aur variance ko assess karne mein madadgar hoti hai aur yeh ensure karta hai ke model generalized hai aur overfitting se bacha hua hai.\n\nAam tor par, jab data limited ho ya model complexity zyada ho, tab cross-validation ka istemal zyada faydemand hota hai. Yeh technique data ke sahi tareeqe se use aur model ki sahi parameter tuning mein bhi madad deti hai.\n\nliken ap big datasets pa bhi good results ke liye esy use kr skty hain.\n\n- Ager ap Interest rkhtay hain es ko detail me smjhanay kii to es link pa jayen aur blog read krien ::\n    - **Cross Validation Blog Link :: https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right**\n    \nKuch Smjh me na aye , So ap Dm kr ky Poch Sktay hain , Me Explain kr dn ga","metadata":{}},{"cell_type":"code","source":"# Es se Bhi Phly Yeh Smjhna Zarori hy ke Data ko Split Kesy Kia Jaye , So meny Ek Achi Website Dhoondhi hy Jis ka Link me Attach kar\n# Rha hn Idhr Ap ne Es Website pa Jaa ke Cross Validation ke Tareeqay Dekh Lenay Hain . Beacuse Idhr me Explain krn ga to \n# Session Lamba ho Jaye ga , Ager Apko Smjh Nahi aye gyii , So Ap Dm me Question krien , me Ap ko Complete Guide kr dn ga.\n\n# i am Using StratifiedKFold in My Session , StratifiedKFold is Much Better Than Train Test Split , You Can Use any other According to you Problem,\n# Link me Complete Guide se Batya Huwa hy ke Kab Kn sa Cross Validation Method Use krna hy.\n\n### StratifiedKFold Istemaal Karna\n\n\"\"\"So StratifiedKFold hy Kia ? \n\nStratified k-Fold ek variation hai standard k-Fold CV technique ka jo target imbalance cases me effective hota hai. \n\nIska kaam kuch is tarah hota hai: Stratified k-Fold dataset ko k folds me is tarah se split karta hai ke har fold me\napproximately same percentage of samples hoti hain har target class ki, jaise ke complete set me hoti hain. Regression\nke case me, Stratified k-Fold yeh ensure karta hai ke har fold me mean target value approximately equal ho.\n\nStratified k-Fold algorithm kuch is tarah hai:\n\n1. Folds ki tadaad ka intekhab karein – k\n2. Dataset ko k folds me split karein. Har fold me approximately same percentage of samples hone chahiyein har target\nclass ki jo ke complete set me hoti hain.\n3. k – 1 folds ko training set ke liye use karein. Baqi ek fold test set hoga.\n4. Training set pe model ko train karein. Har iteration me ek naya model train hona chahiye.\n5. Test set pe validate karein.\n6. Validation ka result save karein.\n7. Steps 3 – 6 ko k martaba repeat karein. Har dafa baqi fold ko test set ke taur pe use karein. Aakhir me aapne har\nfold pe model validate kar liya hoga.\n8. Final score hasil karne ke liye step 6 ke results ko average karein.\"\"\"\n\nfrom sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=N_SPLIT) # Idhr n_Splits ki Value Ham khud Define kr Sktay hain Apni Mrzi se \n\nfor train_index, test_index in skf.split(X_iris, y_iris):\n    X_train_iris, X_test_iris = X_iris.iloc[train_index], X_iris.iloc[test_index]\n    y_train_iris, y_test_iris = y_iris.iloc[train_index], y_iris.iloc[test_index]\n    \nfor train_index, test_index in skf.split(X_tip, y_tip):\n    X_train_tip, X_test_tip = X_tip.iloc[train_index], X_tip.iloc[test_index]\n    y_train_tip, y_test_tip = y_tip.iloc[train_index], y_tip.iloc[test_index]\n\n# ### Smjhana :\n\n# 1. StratifiedKFold Object Banane:\n#    `StratifiedKFold` ek tarah ka KFold hai jo data ko split karta hai, lekin yeh ensure karta hai ke har fold mein har class ke samples ki proportional distribution ho.\n\n# 2. Data Split Karna:\n#    skf.split(X, y)` function `X` (features) aur `y` (labels) ko lekar data ko split karta hai.\n#    Har iteration mein, `train_index` aur `test_index` milte hain jo training aur test sets ke indexes hote hain.\n\n# 3. Training aur Test Data Banane:\n#    X_train` aur `X_test` features ke liye training aur test data hain jo `train_index` aur `test_index` ke basis par select kiya jata hai.\n#    y_train` aur `y_test` labels ke liye training aur test data hain jo unhi indexes ke basis par select kiya jata hai.\n\n# Yeh approach istemaal kar ke, aap data ko stratified manner mein split kar sakte hain, jisse har fold mein har class ke samples ki proportional distribution maintain hoti hai.","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:44:32.600404Z","iopub.execute_input":"2024-06-30T18:44:32.600829Z","iopub.status.idle":"2024-06-30T18:44:32.632916Z","shell.execute_reply.started":"2024-06-30T18:44:32.600794Z","shell.execute_reply":"2024-06-30T18:44:32.631776Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# How to Get The Best Model\nTo find Best Model , Make a Dic Containing all The Models You want to evaluate and the run and loop and train all the model one by one , predict the results , evluate the scores and make a dataframe.\nNow You have all the models with their scores. now you can easily Select the Best Model.\n\nMaan Lo ke hamaray pass dataset bht large hy, ab es case me kia krna chahiye , es case me best and optimal solution hy ke ap data ka sample lyn aur us ko achay se split krien random_seed ke sth , phr us data set pa same upper wala method apply krien aur best model select kr lyn.","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color: #00A4CCFF; font-family: Pacifico, cursive; color: #E6E6FA; font-size: 100%; text-align: left; border-radius: 5px; padding: 10px 30px; display: inline-block;\">1st Way If Data is Small</p>","metadata":{}},{"cell_type":"markdown","source":"## Iris ","metadata":{}},{"cell_type":"code","source":"# Acha to Ab hamaray models import ho gye hain ab ham sb se phly , in models me se best model ko select krien gyen\n# Jo Model sb se acha ho ga us pa ham further Parameter tunning krien gye.\n\n# Best Model ko Kesy Find Kia Jaa Skta hy , Ager Data Set Bht Large ho to tb kesy Best Model ko Nikalna hy , Chalien Sb Se Phly Dekhtay\n# Hain How to Get Best Model\n\n# Sb se Phly Ham ek Dic Bana Lyn Gye Models ki With Their names.\nmodels = {\n    'RF' : RandomForestClassifier(random_state=SEED),\n    'DT' : DecisionTreeClassifier(random_state=SEED),\n    'XGB': XGBClassifier(random_state=SEED),\n    'CAT': CatBoostClassifier(verbose=0,random_state=SEED), # Idhr me Verbose 0 or -1 is liye pass kr rha hn ku ke Output ko Truncate kr rha \n    'LGBM': LGBMClassifier(verbose=-1,random_state=SEED) # hn CatBoost aur LGBM ki \n}\n\n# Ab Hamara Next Step ho ga Ham ek Loop Run Krien Gye Jo ke Hamarai Dic me se Model aur un ke Names ko Pick Kry ga aur Phr One \n# by one Un Models ko Train kr Ky Un ke Resultsn ko Store kry ga Then ham un Results ko ek Dataframe me Store Krwaien Gye. \n\n# Idhr ab ek List bhi Intilize krien gye Jis me Hamary Scores aur Model ka Name Store Hota Rahe \nScores_of_Models = []\n\n# Lets Run The Loop.\nfor name,model in models.items():  # .items() use karna zaroori hai\n    M = model\n    \n    # Train Model \n    M.fit(X_train_iris,y_train_iris)\n    \n    # Predict \n    y_Pred = M.predict(X_test_iris)\n    \n    # Idhr me Bs Accuracy is Check krn ga , Ap Apni Problem ke Hisab se Kr Sktay Hain Kuch aur Matrix add\n    acc = accuracy_score(y_test_iris,y_Pred)\n    \n    Scores_of_Models.append((name,acc))\n    \n    # Print The Score \n#     print(f'Accuracy of The {name} is {acc}')\n    \n# Ab Jo Scores Ayen Hain Ham Un Ko Ek Data Frame me Ly Jayen gye aur Show Krwaien Gye \nscores_df = pd.DataFrame(Scores_of_Models, columns=['Model', 'Accuracy'])\nscores_df","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:44:35.231439Z","iopub.execute_input":"2024-06-30T18:44:35.232294Z","iopub.status.idle":"2024-06-30T18:44:36.345591Z","shell.execute_reply.started":"2024-06-30T18:44:35.232259Z","shell.execute_reply":"2024-06-30T18:44:36.344423Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"  Model  Accuracy\n0    RF  0.933333\n1    DT  0.866667\n2   XGB  1.000000\n3   CAT  1.000000\n4  LGBM  0.733333","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>RF</td>\n      <td>0.933333</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DT</td>\n      <td>0.866667</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>XGB</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CAT</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LGBM</td>\n      <td>0.733333</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"- **Ab,Farak dekhien , Cross Validation ko Use krty Huway Model ko Evaluate krny pa Ap dekh sektay hain Results me kitna change aa gya hy. Jo Model Phly 100% acc de rhy thy ab woh Kitni Kam accuracy de rhy Hain.**\n\n- **Jab Apke Pass Dataset Small ho to usko Cross-Validate Lazmi krien.**","metadata":{}},{"cell_type":"markdown","source":"## Tips","metadata":{}},{"cell_type":"code","source":"# Acha to Ab hamaray models import ho gye hain ab ham sb se phly , in models me se best model ko select krien gyen\n# Jo Model sb se acha ho ga us pa ham further Parameter tunning krien gye.\n\n# Best Model ko Kesy Find Kia Jaa Skta hy , Ager Data Set Bht Large ho to tb kesy Best Model ko Nikalna hy , Chalien Sb Se Phly Dekhtay\n# Hain How to Get Best Model\n\n# Sb se Phly Ham ek Dic Bana Lyn Gye Models ki With Their names.\nmodels = {\n    'RF' : RandomForestClassifier(random_state=SEED),\n    'DT' : DecisionTreeClassifier(random_state=SEED),\n    'XGB': XGBClassifier(random_state=SEED),\n    'CAT': CatBoostClassifier(verbose=0,random_state=SEED), # Idhr me Verbose 0 or -1 is liye pass kr rha hn ku ke Output ko Truncate kr rha \n    'LGBM': LGBMClassifier(verbose=-1,random_state=SEED) # hn CatBoost aur LGBM ki \n}\n\n# Ab Hamara Next Step ho ga Ham ek Loop Run Krien Gye Jo ke Hamarai Dic me se Model aur un ke Names ko Pick Kry ga aur Phr One \n# by one Un Models ko Train kr Ky Un ke Resultsn ko Store kry ga Then ham un Results ko ek Dataframe me Store Krwaien Gye. \n\n# Idhr ab ek List bhi Intilize krien gye Jis me Hamary Scores aur Model ka Name Store Hota Rahe \nScores_of_Models = []\n\n# Lets Run The Loop.\nfor name,model in models.items():  # .items() use karna zaroori hai\n    M = model\n    \n    # Train Model \n    M.fit(X_train_tip,y_train_tip)\n    \n    # Predict \n    y_Pred = M.predict(X_test_tip)\n    \n    # Idhr me Bs Accuracy is Check krn ga , Ap Apni Problem ke Hisab se Kr Sktay Hain Kuch aur Matrix add\n    acc = accuracy_score(y_test_tip,y_Pred)\n    \n    Scores_of_Models.append((name,acc))\n    \n    # Print The Score \n#     print(f'Accuracy of The {name} is {acc}')\n    \n# Ab Jo Scores Ayen Hain Ham Un Ko Ek Data Frame me Ly Jayen gye aur Show Krwaien Gye \nscores_df = pd.DataFrame(Scores_of_Models, columns=['Model', 'Accuracy'])\nscores_df","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:44:37.848480Z","iopub.execute_input":"2024-06-30T18:44:37.849423Z","iopub.status.idle":"2024-06-30T18:44:39.108673Z","shell.execute_reply.started":"2024-06-30T18:44:37.849329Z","shell.execute_reply":"2024-06-30T18:44:39.107616Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"  Model  Accuracy\n0    RF  0.541667\n1    DT  0.458333\n2   XGB  0.583333\n3   CAT  0.583333\n4  LGBM  0.666667","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>RF</td>\n      <td>0.541667</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DT</td>\n      <td>0.458333</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>XGB</td>\n      <td>0.583333</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CAT</td>\n      <td>0.583333</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LGBM</td>\n      <td>0.666667</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# <p style=\"background-color: #00A4CCFF; font-family: Pacifico, cursive; color: #E6E6FA; font-size: 100%; text-align: left; border-radius: 5px; padding: 10px 30px; display: inline-block;\">2nd Way If Data is Large</p>","metadata":{}},{"cell_type":"code","source":"\"\"\"Es ke Liye sab se phly ham ek data load krien gye Jo ke diamonds ka data hy , woh large Dataset hy , Jo ke Zayda \n# Models pa Kaffi Zayda Time Ly Jaye ga So Us ko Handle Krien gye Ham Idhr aur Us Same code pa dekhien gye ke kesy\nham problem handle krty hain\"\"\"\ndiamond = sns.load_dataset('diamonds')\nprint(diamond.head())\nprint(\"------------------------------------\")\n\"\"\"Ab Es data ki Shape bhi dekh letay hain ek br\"\"\"\nprint(f'The Shape Of Data is {diamond.shape}')\nprint(f'The No Of Rows in Our Data is {diamond.shape[0]}')\n\n\"\"\"Jesa ka ab hamara data kaafi large hy , ab ager ham es pa wohi code run krien gye to , yeh Time bht Zayda Consume kry ga,\nso ab es Situation me kia krna chahiye, Ab Es situation me ham data ka sample ly ln gye randomseed ke sth takay data \nsahi se shuffle ho ke aye aur modeling effect na ho\n\nUs se bhi Phly Hamy data ko preprocess krna ho ga , takay modeling me koi error na aye.\n\nHow to Preprocess Data for Machine Learning. es me Kuch setps hain Jo Ham one by One Idhr Dekhien gye.\n\nA. Sab se Phly Ham Data me Check krien Gye Ke Koi Null Value to Nahi Naw?\"\"\"\nprint('--------------------------------------------------')\nprint(f\"Null Values in Data is \\n {diamond.isnull().sum()}\")\nprint('--------------------------------------------------')\n\n\"\"\"So hamara data null Values se to bacha huwa hy mean ek step clear hy , chalien Sochien ager yahan pa null values \nhoti to ham unhien kesy handle krty ? \n\nNull Values ko Handle krny ke liye bht se different methods hain un me se kuch Advance hain Kuch Beginners hain\n\nNull Values ko Bht dehan se handle krna parta hy , yeh Machine learning pa bht effect krti hain. Ager Ham Null Values ko Sahi se \nHandle nahi krty to hamara model glt predictions bhi de skta hy ya model achi performence tk nahi ponchta.\n\nChalien Sb se Phly Kuch Beginners Level Ke Method ko Discuss krty hain\n\n1. Mean\n2. Median\n3. Mode\n\nYeh 3 Method hain Jo Basically Beginner level pa use hotay, Like Yen Methods Sahi nahi ager data me ek large scale pa \ndata missing hai\nLarge scale pa jab in methods ko use kia jaye to yeh data ko bht bura affect krty hain aur may be model ki performence\nbht krb ho jaye\n\nAb Kuch Advance Methods ki Baat Kr Letay hain..\n\n1. Iterative Imputer for Numerical Values :: Iterative Imputer ek behtareen method hai numerical null values ko handle\nkarne ka. Agar data mein large scale par null values hain, to aap is method se impute kar sakte hain. Is method mein, \nmissing values ko multiple regression models ke through predict karke fill kiya jata hai. Har feature ko doosre features \nke basis par estimate kiya jata hai. \n\n    Algorithm ka overview Iterative Imputer ek iterative process use karta hai. Pehle, missing values ko kuch initial\n    guesses se fill kiya jata hai (jaise mean se). Phir, har feature ko ek regression model banaya jata hai jisme baaki\n    features predictors ke tor par use hote hain. Ye process tab tak repeat hota hai jab tak imputations stabilize nahi\n    ho jati.\n\n    Advantage Ye method missing values ko zyada accurately fill karta hai kyunki har feature ko baaki features ke context\n    mein consider kiya jata hai. Ye method zyada tar situations mein accurate predictions provide karta hai aur model \n    performance ko improve karta hai.\n\n    Experience Mere experience ke mutabiq, ye best method hai agar aapke data mein large scale par numerical null values\n    hain. Ye method data ke pattern ko achi tarah capture karta hai aur realistic imputations provide karta hai.\n\nNote: Aap yeh method use karne se pehle ensure karen ke aapke paas sufficient computational resources hain, kyunki yeh\nmethod computationally intensive ho sakta hai, especially large datasets ke liye\n\nAlso You can use other Mehtod instead of iteravtive imputer , like rf , other tree methods , Woh bhi Null Values ko achay\nse predict kr ky impute krty hain\n\n\n2. Agar data mein ek large scale par Categorical Null Values hain :: to sab se best option hai ke aap usko domain\nknowledge ki base par impute karein. Agar aapke paas domain knowledge nahi hai, to phir aapko kuch advanced methods \nki taraf jana chahiye, jaise model predictions.\n\n    Domain Knowledge: Pehle step mein, domain knowledge ko use karte hue missing values ko fill karna ideal hota hai. \n    Example ke tor par, agar aapke pass customer data hai aur gender missing hai, to aap industry trends ya specific \n    business context ke basis par guess kar sakte hain.\n\n    Model Predictions**: Agar domain knowledge available nahi hai, to aap advanced methods use kar sakte hain jaise \n    tree models (decision trees, random forests) ya gradient boosting models. Ye models categorical values ko accurately\n    predict kar sakte hain. Iska ek complete code bhi mein aapke saath share kar dunga. However, ye method time-consuming hai agar data large ho. Lekin agar aapke paas achi computational resources hain, to aap is method ko try kar sakte hain.\n\n    Constants se Imputation: Ek aur best way imputation ka yeh hai ke aap constants use karein jaise \"No\", \"None\", \n    \"No record\", etc. Ye method maine kaafi dafa use kiya hai aur isse model ki performance bhi achi hui hai. \n    Ye method simple hai aur computationally efficient bhi.\n\n    Advice : Hamesha ek hi flow par rely nahi karna chahiye. Aapko mukhtalif methods se testing karni chahiye\n    aur jo best lage usi ko aage leke jana chahiye. Ek best model hasil karne ke liye har technique ko try out\n    karna zaroori hai.\n\nYeh kuch tareeqay hain jo ke null values ko impute karne ke liye zaroori hain.\n\n---------------------------------------------------------------------------------------------------------------------------\nB. Outliers\n\nNull values ke baad hum data mein outliers ko dekhenge. Outliers data ko bohot bura effect karte hain aur saath hi saath\nmodels ko bhi effect karte hain. Outliers kabhi kabhi faida bhi dete hain aur nuqsan bhi.\n\nOutliers ko handle karna zaroori hai aur kuch models itne robust hote hain jo ke outliers ko khud hi deal kar lete hain.\nIn models ke names hain:\n\nDecision Trees :: Decision trees naturally segment data based on feature values aur ye outliers se kam effect hote hain.\nRandom Forests :: Random forests bhi decision trees ka ensemble hote hain, isliye ye bhi outliers ke liye robust hote hain.\nGradient Boosting Machines (GBM) :: GBMs bhi outliers ko kuch had tak handle kar sakte hain apni iterative nature aur\nmisclassified examples par focus karne ki wajah se.\nSupport Vector Machines (SVM) :: SVMs bhi outliers ke liye robust ho sakte hain agar appropriate kernel aur parameters \nchoose kiye jayen.\nRobust Regression Models :: Models jaise Huber Regression aur RANSAC (Random Sample Consensus) specifically outliers ke \nliye robust design kiye gaye hain.\n\nOutliers ko handle karne ke liye aap different techniques use kar sakte hain, jaise:\n\n1. Trimming/Removing Outliers: Outliers ko data se nikal dena.\n2. Transforming Data: Log transformation ya other transformations use karna.\n3. Capping: Outliers ko upper ya lower bounds par cap kar dena. Capping me IQR , Z_Score Jesy Method use Hotay hain\n4. Imputation: Outliers ko mean, median, ya mode se replace karna.\n\nKuch specific use cases mein outliers ko maintain karna bhi beneficial ho sakta hai, especially agar wo data ke specific\ncharacteristics ko represent karte hain.\n\nBaqi Ager Kisi ne es ko Detail ne krna ho to, You can check this Video :: 1. https://youtu.be/Lln1PKgGr_M?si=DGi1huwW1q0njFYj\n2. https://youtu.be/OnPE-Z8jtqM?si=eFMKekI9dIFdRaNf\n3. https://youtu.be/Ccv1-W5ilak?si=k6WNgh_dlhgGhywb\n4. https://youtu.be/bcXA4CqRXvM?si=ifJJqYBmnNiTPnuG\n\nYeh 4 Videos Dekh Sktay Hain Ager Mazeed Information Leni hy to.\n\n-----------------------------------------------------------------------------------------------------------------------------\nC. Scaling\n\nScaling machine learning preprocessing ka important part hai. Lekin kuch models hain jo ke scaling se \nrobust hain aur un par scaling ka baghair bhi kuch zyada effect nahi hota. Yeh models hain:\n\nDecision Trees: Decision trees scaling se mutasir nahi hote.\nRandom Forests: Random forests bhi scaling se robust hain.\nGradient Boosting Machines (GBM): GBMs bhi scaling se itna effect nahi hote.\nNaive Bayes: Naive Bayes scaling ka muhtaaj nahi hota.\nK-Nearest Neighbors (KNN): KNN scaling se directly mutasir ho sakta hai, lekin distance metric ko modify kar ke isko \nrobust bana sakte hain.\nSupport Vector Machines (SVM) with RBF or Polynomial Kernels: SVMs with certain kernels scaling se kam effect hoti hain.\nTree-based Ensemble Methods: Bagging aur boosting methods jaise AdaBoost scaling se robust hain.\n\nScaling ke Different Methods hain \n\nMin-Max Scaling: Is method mein data ko ek specific range (usually 0 se 1) mein scale kiya jata hai. Yeh method outliers\nke liye sensitive hota hai.\n\nStandard Scaling: Is method mein data ko aise scale kiya jata hai ke uska mean 0 aur standard deviation 1 ho jaye. \nYeh method zyada tar cases mein use hota hai.\n\nRobust Scaling: Is method mein data ko aise scale kiya jata hai ke uske median aur interquartile range (IQR) ke basis \npar scaling ho. Yeh method outliers ke liye zyada robust hota hai.\n\nKn se Method kab Chose krna hy Es ki Koi ek Specific se Limit nahi ap as an Experiment bhi esy kr skty ho.\nBaqi For More Deep Knwodlge Me Kuch Links Share kr rha hn Inhien Dekh lena \n1. https://youtu.be/eBrGyuA2MIg?si=6XxDh8HU3Y55zlbK\n2. https://youtu.be/1Yw9sC0PNwY?si=TIA92CiBgGnO2y0I\n\nNote :: Wesy Jahan hamy ek value ki Limit Pata ho wahan ham MinMax ko use krty hain aur Jahan Value ki Range ka Andaza na ho to Ham\nStandarad Scaler ko Use krty Hain\n\nChalo ab Data ko Scale Krty hain. So Meny Ek Code Likh ke Rkha huwa hy Jis ne Scaling ki hr Command hy Bs ap ne Apni Choice \nProvide krni hy aur data ne scale ho jana hy. \n---------------------------------------------------------------------------------------------------------------------------\n\"\"\"\n# Import The Scalers \nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, QuantileTransformer, PowerTransformer\n\n# Function to Scale Data\ndef apply_scaling(data, columns, scaler_type):\n    # Check the type of scaler and initialize the appropriate scaler object\n    if scaler_type == 'S':\n        scaler = StandardScaler()  # Initialize StandardScaler\n    elif scaler_type == 'M':\n        scaler = MinMaxScaler()  # Initialize MinMaxScaler\n    elif scaler_type == 'R':\n        scaler = RobustScaler()  # Initialize RobustScaler\n    elif scaler_type == 'A':\n        scaler = MaxAbsScaler()  # Initialize MaxAbsScaler\n    elif scaler_type == 'Q':\n        scaler = QuantileTransformer(output_distribution='normal')  # Initialize QuantileTransformer\n    elif scaler_type == 'P':\n        scaler = PowerTransformer()  # Initialize PowerTransformer\n    else:\n        raise ValueError(\"Invalid scaler type. Choose 'S' for StandardScaler, 'M' for MinMaxScaler, 'R' for RobustScaler, 'A' for MaxAbsScaler,'Q' for QuantileTransformer, or 'P' for PowerTransformer.\")\n\n    # Create a copy of the input data to avoid modifying the original data\n    scaled_data = data.copy()\n\n    # Loop through each column to be scaled\n    for col in columns:\n        # Apply the scaler to the current column and update the data with the scaled values\n        scaled_data[col] = scaler.fit_transform(scaled_data[[col]])\n\n    # Return the scaled data\n    return scaled_data\n\n# Specify columns and scaler type\ncolumns_to_scale = [col for col in diamond.columns if diamond[col].dtype == 'float']\nscaler_type = 'S' \n\n# Apply scaling data\ndiamond = apply_scaling(diamond, columns_to_scale, scaler_type)\nprint('Training data scaled successfully.')\nprint('--------------------------------------------------')\n\n\n\"\"\"\n---------------------------------------------------------------------------------------------------------------------------\nD. Encoding\nData ko encode karna bhi bohot important step hai kyunki computers sirf numeric form mein data samajhte hain. Data ko\nencode karne ke different methods hain:\n\nOne-Hot Encoding (OHE): Yeh method har category ka ek alag column bana deta hai. Yeh tab useful hota hai jab categories \nmein kisi tarah ka order nahi hota.\n\nLabel Encoding: Yeh method categories ko unique numeric values assign karta hai. Yeh tab use hota hai jab categories mein\nkisi tarah ka natural order hota hai.\n\nOrdinal Encoding: Yeh method bhi categories ko numeric values assign karta hai, lekin aap khud se order decide kar sakte\nhain. Yeh tab useful hota hai jab categories ke beech mein ek specific order hota hai jo aap define karna chahte hain.\n\nmeny es ka bhi function bana rkha hy , jo ke Apke bht Kam aa skta hy. \n\"\"\"\n# Function to Encode Data\ndef encode(data, columns, encoding_type='label'):\n    try:\n        # Initialize encoders\n        label_encoder = LabelEncoder()\n        one_hot_encoder = OneHotEncoder(sparse=False, drop='first')\n        \n        # Idhr ham Data ki Copy bana Lyn Gye\n        encoded_data = data.copy()\n        \n        for col in columns:\n            if encoding_type == 'label':\n                # Apply Label Encoding\n                encoded_data[col] = label_encoder.fit_transform(data[col])\n            elif encoding_type == 'onehot':\n                # Idhr Ham Ohe Ko Apply krien gye Sth Sth New Transform Cols aur Dusray Cols ko Combine krien gye\n                one_hot_encoded = one_hot_encoder.fit_transform(data[[col]])\n                one_hot_encoded_df = pd.DataFrame(one_hot_encoded, columns=[f\"{col}_{category}\" for category in label_encoder.fit(data[col]).classes_[1:]])\n                encoded_data = pd.concat([encoded_data.drop(col, axis=1), one_hot_encoded_df], axis=1)\n            else:\n                raise ValueError(\"Invalid encoding type. Choose 'label' for Label Encoding or 'onehot' for One-Hot Encoding.\")\n        \n        return encoded_data\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n# Call \ncolumns_to_encode = ['color','clarity','cut']\nencoding_type = 'label'\n\n# Apply encoding to the data\ndiamond = encode(diamond, columns_to_encode, encoding_type)\nprint('Data Encoded')\nprint('--------------------------------------------------')\n\n\n#--------------------------------------------------------------------------------------------------------------------------\n\"\"\"\nAp Hamara Data Machine Learning ke Liye Bilkul Ready hy , ap ham Apnay Models ko apply kr Sktay Hain but Main Question , Data \nbht Bhara hy , Bht time ly gyii Training to Kia Kiya Jaye , Ab Ham Data ka Random seed ke Sth Sample Lyn Gye \n\n\"\"\"\n# Ap Kitnay Sample lena Chahtay hain Us ke Mutaqib Idhr Number add Kr Lyn, Jesy me Ihdr bs 10,000 Sample le rha hn\nn_samples = 2000\n# Ab Data ko Sample kr Lyn gye\ndiamond_sampled = diamond.sample(n=n_samples,random_state=SEED)\nprint(f\"The Shape Of Sampled Df is {diamond_sampled.shape}\")\nprint('--------------------------------------------------')\n\n\n\"\"\"\nAb ham apnay data ko again X and y me divide krien gye Then Us ko Train and Test me Cross validate Split krien gye \nand phr Model Build krien gye\n\"\"\"\nX = diamond_sampled.drop('cut',axis=1)\ny = diamond_sampled['cut']\n\n\"\"\"Es se Bhi Phly Yeh Smjhna Zarori hy ke Data ko Split Kesy Kia Jaye , So meny Ek Achi Website Dhoondhi hy Jis ka Link \nme Attach karRha hn Idhr Ap ne Es Website pa Jaa ke Cross Validation ke Tareeqay Dekh Lenay Hain . Beacuse Idhr \nme Explain krn ga to Session Lamba ho Jaye ga , Ager Apko Smjh Nahi aye gyii , So Ap Dm me Question krien , me Ap ko \nComplete Guide kr dn ga.\n\n# i am Using StratifiedKFold in My Session , StratifiedKFold is Much Better Than Train Test Split ,\nYou Can Use any other According to you Problem,Here Our Problem is Multiclass that Why I use This\n# Link me Complete Guide se Batya Huwa hy ke Kab Kn sa Cross Validation Method Use krna hy.\n\n### StratifiedKFold Istemaal Karna\n\n\"\"\"\n\nfrom sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=N_SPLIT) # Idhr n_Splits ki Value Ham khud Define kr Sktay hain Apni Mrzi se \n\nfor train_index, test_index in skf.split(X, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n\"\"\"Smjhana :\n\n1. StratifiedKFold Object Banane:\n    `StratifiedKFold` ek tarah ka KFold hai jo data ko split karta hai, lekin yeh ensure karta hai ke har \n    fold mein har class ke samples ki proportional distribution ho.\n\n2. Data Split Karna:\n    skf.split(X, y)` function `X` (features) aur `y` (labels) ko lekar data ko split karta hai.\n    Har iteration mein, `train_index` aur `test_index` milte hain jo training aur test sets ke indexes hote hain.\n\n3. Training aur Test Data Banane:\n    X_train` aur `X_test` features ke liye training aur test data hain jo `train_index` aur `test_index` ke basis par \n    select kiya jata hai.\n    y_train` aur `y_test` labels ke liye training aur test data hain jo unhi indexes ke basis par select kiya jata hai.\n\n# Yeh approach istemaal kar ke, aap data ko stratified manner mein split kar sakte hain, jisse har fold mein har class ke\nsamples ki proportional distribution maintain hoti hai.\n\n_________________________________________________________________________________________________________________________\n\nAcha to Ab hamaray models import ho gye hain ab ham sb se phly , in models me se best model ko select krien gyen\nJo Model sb se acha ho ga us pa ham further Parameter tunning krien gye.\n\nBest Model ko Kesy Find Kia Jaa Skta hy , Ager Data Set Bht Large ho to tb kesy Best Model ko Nikalna hy , Chalien Sb Se Phly Dekhtay\nHain How to Get Best Model\n\"\"\"\n\n# Sb se Phly Ham ek Dic Bana Lyn Gye Models ki With Their names.\nmodels = {\n    'RF' : RandomForestClassifier(random_state=SEED),\n    'DT' : DecisionTreeClassifier(random_state=SEED),\n    'XGB': XGBClassifier(random_state=SEED),\n    'CAT': CatBoostClassifier(verbose=0,random_state=SEED), # Idhr me Verbose 0 or -1 is liye pass kr rha hn ku ke Output ko Truncate kr rha \n    'LGBM': LGBMClassifier(verbose=-1,random_state=SEED) # hn CatBoost aur LGBM ki \n}\n\n# Ab Hamara Next Step ho ga Ham ek Loop Run Krien Gye Jo ke Hamarai Dic me se Model aur un ke Names ko Pick Kry ga aur Phr One \n# by one Un Models ko Train kr Ky Un ke Resultsn ko Store kry ga Then ham un Results ko ek Dataframe me Store Krwaien Gye. \n\n# Idhr ab ek List bhi Intilize krien gye Jis me Hamary Scores aur Model ka Name Store Hota Rahe \nScores_of_Models = []\n\n# Lets Run The Loop.\nfor name,model in models.items():  # .items() use karna zaroori hai\n    M = model\n    \n    # Train Model \n    M.fit(X_train,y_train)\n    \n    # Predict \n    y_Pred = M.predict(X_test)\n    \n    # Idhr me Bs Accuracy is Check krn ga , Ap Apni Problem ke Hisab se Kr Sktay Hain Kuch aur Matrix add\n    acc = accuracy_score(y_test,y_Pred)\n    \n    Scores_of_Models.append((name,acc))\n    \n    # Print The Score \n#     print(f'Accuracy of The {name} is {acc}')\n    \n# Ab Jo Scores Ayen Hain Ham Un Ko Ek Data Frame me Ly Jayen gye aur Show Krwaien Gye \nscores_df = pd.DataFrame(Scores_of_Models, columns=['Model', 'Accuracy'])\nscores_df","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:44:42.338075Z","iopub.execute_input":"2024-06-30T18:44:42.338477Z","iopub.status.idle":"2024-06-30T18:44:50.569607Z","shell.execute_reply.started":"2024-06-30T18:44:42.338444Z","shell.execute_reply":"2024-06-30T18:44:50.568452Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"   carat      cut color clarity  depth  table  price     x     y     z\n0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n3   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n4   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75\n------------------------------------\nThe Shape Of Data is (53940, 10)\nThe No Of Rows in Our Data is 53940\n--------------------------------------------------\nNull Values in Data is \n carat      0\ncut        0\ncolor      0\nclarity    0\ndepth      0\ntable      0\nprice      0\nx          0\ny          0\nz          0\ndtype: int64\n--------------------------------------------------\nTraining data scaled successfully.\n--------------------------------------------------\nData Encoded\n--------------------------------------------------\nThe Shape Of Sampled Df is (2000, 10)\n--------------------------------------------------\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"  Model  Accuracy\n0    RF     0.730\n1    DT     0.615\n2   XGB     0.765\n3   CAT     0.740\n4  LGBM     0.760","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>RF</td>\n      <td>0.730</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DT</td>\n      <td>0.615</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>XGB</td>\n      <td>0.765</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CAT</td>\n      <td>0.740</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LGBM</td>\n      <td>0.760</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# <p style=\"background-color: #00A4CCFF; font-family: Pacifico, cursive; color: #E6E6FA; font-size: 100%; text-align: left; border-radius: 5px; padding: 10px 30px; display: inline-block;\">How to Tune Model || Optuna Introduction</p>\n\nParametron ka sahi tashkeel model ke liye bohat ahmiyat rakhti hain. Agar hum model ke behtareen parametron ko dhoondh lein to hum ek behtareen model hasil kar sakte hain. Ab sawal yeh hai ke hum behtareen parametron ko kaise hasil kar sakte hain? Behtareen parametron ko dhoondhne ko hum hyperparameter tuning kehte hain.\n\nHyperparameter tuning ek aisi prakriya hai jisme hum model ke parametron ko optimize karne ka tareeqa tajweez karte hain. Is mein Optuna naam ka ek framework bohat zyada mashhoor hai. Optuna machine learning engineers aur data scientists ke darmiyan popular hai kyunki yeh ek automatic hyperparameter optimization framework hai.\n\nOptuna ke zariye, hum model ke liye behtareen hyperparametron ko dhoondhne ke liye algorithmic search ka istemal karte hain. Is mein hum bas kuch initial parameters (jaise ke learning rate, batch size, ya tree depth) provide karte hain, aur Optuna khud baqi kaam karta hai. Yeh har ek parameter ko ek ek karke try karta hai, har experiment ke natijay mein model ki performance ko evaluate karta hai, aur sab se behtareen hyperparametron ko suggest karta hai jo model ke liye optimize hotay hain.\n\nOptuna ki flexibility aur automation ki wajah se, yeh aam tor par machine learning projects mein istemal kiya jata hai, jahan kehli wajah se is ke zariye complex models ko bhi efficiently optimize kiya ja sakta hai.\n\nAger Ap Optune ko Mazeed study krna chahien to ap optune ki Documentation check kr skty hain link me attach kr dn ga.\n- Optuna :: https://optuna.org/","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color: #00A4CCFF; font-family: Pacifico, cursive; color: #E6E6FA; font-size: 100%; text-align: left; border-radius: 5px; padding: 10px 30px; display: inline-block;\">RF Params Tunning</p>","metadata":{}},{"cell_type":"markdown","source":"**`n_trials` Optuna ko batata hai ke kitne alag-alag trials (experiments) run karne hain. Har trial mein, Optuna alag-alag parameter combinations ko evaluate karega aur best parameters ko dhundega.**\n\n**`trial` ek individual trial ko represent karta hai jo objective function ke andar pass hota hai. trial object ke through, aap parameters ko suggest (recommend) kar sakte hain jo us specific trial mein use hone hain. Yeh trial object parameters ko sample karta hai specified ranges aur distributions ke andar.**\n\n**Iska matlab hai ke n_trials=20 ka matlab hai ke Optuna 20 trials run karega, aur har trial mein ek trial object pass kiya jayega jo naye parameters suggest karega.**","metadata":{}},{"cell_type":"code","source":"# Kuch Model ki Performence bht Achy hy Kuch ki Bs Normal hy , Zayda achi nahi ab ham esy krien gye RF ko Lyn gye aur Optuna se \n# Params Tunned krien Gye aur Check Krien Gye ke Kia Koi Improvement aa rhi hy Score me ? \n\n# Import kr lyn Optuna ko \nimport optuna\n\n# Ap Optuna ka Code Likhtay Hain aur Usy Use Krty Hain , Ham bs 20 Trails pa Results ko Check Krien Gye aur Dekhien Gye \n# Params ko Optimized Krny ke Baad Kia Score ata hy \n\ndef objective(trial):\n    \n    \"\"\"Objective function ke andar, sabse pehle parameters define kiye jate hain jo optimize hone hain.\n    Yeh parameters trial object ke methods se suggest kiye jate hain, jo ki search space ko define karte hain.\"\"\"\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),  # Number of trees in the forest.\n        'max_depth': trial.suggest_int('max_depth', 2, 50),  # Maximum depth of each tree.\n        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),  # Minimum samples required to split an internal node.\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),  # Minimum samples required to be a leaf node.\n        'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),  # Number of features to consider when looking for the best split.\n        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),  # Whether bootstrap samples are used when building trees.\n        'random_state': SEED,\n    }\n    \n    val_scores = []\n    \n    \"\"\"Ek StratifiedKFold object create kiya jata hai jisse data ko n splits mein divide kiya jata hai aur ensure\n    kiya jata hai ki har split mein class distribution consistent ho.\"\"\"\n    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    \"\"\"For loop ke through, har split par training aur validation data define kiya jata hai. Phir model train kiya jata \n    hai aur validation set par predict kiya jata hai. Har trial ke validation accuracy ko store kiya jata hai.\"\"\"\n    for train_index, test_index in skf.split(X_tip, y_tip):\n        X_train_tip, X_test_tip = X_tip.iloc[train_index], X_tip.iloc[test_index]\n        y_train_tip, y_test_tip = y_tip.iloc[train_index], y_tip.iloc[test_index]\n\n        model = RandomForestClassifier(**params)\n        model.fit(X_train_tip, y_train_tip)\n\n        y_test_pred = model.predict(X_test_tip)\n\n        val_acc = accuracy_score(y_test_tip, y_test_pred)\n        val_scores.append(val_acc)\n        \n    \"\"\"Har split ke baad, mean validation accuracy calculate ki jati hai aur return ki jati hai. \n    Optuna is value ko use karta hai parameter set ke performance ko evaluate karne ke liye.\"\"\"\n    mean_val_acc = np.mean(val_scores)\n    return mean_val_acc\n\n    \"\"\"Optimize method ko 20 trials ke liye call kiya jata hai. Har trial mein Optuna alag parameter \n    values try karta hai aur objective function ko call karta hai. Objective function\n    return karta hai accuracy score, jo Optuna use karta hai best parameter set ko identify karne ke liye.\"\"\"\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\nbest_params = study.best_params\nprint(\"Best parameters:\", best_params)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:44:14.391762Z","iopub.status.idle":"2024-06-30T18:44:14.393600Z","shell.execute_reply.started":"2024-06-30T18:44:14.393313Z","shell.execute_reply":"2024-06-30T18:44:14.393342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# After 20 trails , ham ne Yeh Paramsters Hasil Kiye Hain Hain \n# {'n_estimators': 827, 'max_depth': 20, 'min_samples_split': 18, 'min_samples_leaf': 18, 'max_features': 0.615820169162841, 'bootstrap': True}\n\n# Aur in Params pa Hamaray Model ka Score aa rha hy 1 , Jo ke ek Bht Amazing Change aya hy , Its means Optuna is Very \n# Good Option for Params Tunnings.\n\n# Chalien ek br Phr se apna Upper Wala Code Run KRty Hain , Aur Dekhaty Hain Kia Improvement ati hy \n\n\n# Idhr ham Tunned Params ko Use kRien Gye \nT_Params = {'n_estimators': 363, 'max_depth': 15, 'min_samples_split': 2,\n            'min_samples_leaf': 15, 'max_features': 0.9398700877827628, 'bootstrap': False}\n\n# Yeh Params Tips ke 20 Trials ki Improvement ke Hain \nTips_P =  {'n_estimators': 81, 'max_depth': 31, 'min_samples_split': 16, \n           'min_samples_leaf': 15, 'max_features': 0.16076546012392506, 'bootstrap': True}\n\n# Idhr ham ek Code Define Krien Gye Jo Ke Fold Me Check Kry ga aur end pa Mean Score Return kr dey ga \ndef Train_ML(model,Type):\n    \n    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED)\n    train_scores = []\n    val_scores = []\n    \n    if Type == 'I':\n        \n        # # X and Y ko Alg Kr Lyn Ek br \n        X = df.drop('Species',axis=1)\n        y = df['Species']\n\n        for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n            X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n            y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n\n            # Train the model\n            model.fit(X_train, y_train)\n\n            # Make predictions on training data\n            y_train_pred = model.predict(X_train)\n            train_accuracy = accuracy_score(y_train, y_train_pred)\n            train_scores.append(train_accuracy)\n\n            # Make predictions on validation data\n            y_val_pred = model.predict(X_val)\n            val_accuracy = accuracy_score(y_val, y_val_pred)\n            val_scores.append(val_accuracy)\n\n            # Print accuracy for this fold\n            print(f\"Fold {fold}: Train Accuracy = {train_accuracy:.4f}, Validation Accuracy = {val_accuracy:.4f}\")\n\n        # Calculate mean accuracy\n        mean_train_accuracy = np.mean(train_scores)\n        mean_val_accuracy = np.mean(val_scores)\n\n        print(f\"\\nMean Train Accuracy: {mean_train_accuracy:.4f}\")\n        print(f\"Mean Validation Accuracy: {mean_val_accuracy:.4f}\")\n        \n    if Type == 'T':\n        \n        # # X and Y ko Alg Kr Lyn Ek br \n        X = tip.drop('sex',axis=1)\n        y = tip['sex']\n        \n        for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n            X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n            y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n\n            # Train the model\n            model.fit(X_train, y_train)\n\n            # Make predictions on training data\n            y_train_pred = model.predict(X_train)\n            train_accuracy = accuracy_score(y_train, y_train_pred)\n            train_scores.append(train_accuracy)\n\n            # Make predictions on validation data\n            y_val_pred = model.predict(X_val)\n            val_accuracy = accuracy_score(y_val, y_val_pred)\n            val_scores.append(val_accuracy)\n\n            # Print accuracy for this fold\n            print(f\"Fold {fold}: Train Accuracy = {train_accuracy:.4f}, Validation Accuracy = {val_accuracy:.4f}\")\n\n        # Calculate mean accuracy\n        mean_train_accuracy = np.mean(train_scores)\n        mean_val_accuracy = np.mean(val_scores)\n\n        print(f\"\\nMean Train Accuracy: {mean_train_accuracy:.4f}\")\n        print(f\"Mean Validation Accuracy: {mean_val_accuracy:.4f}\")\n\n    return model, train_scores, val_scores\n\n# Ab Ham Apnay Model ko Check krty hain ke Kia , Params Ke Tunned krny se Kuch Faida Huwa ya nahi\nmodel = RandomForestClassifier(random_state=SEED,**T_Params)\nmodel2 = RandomForestClassifier(random_state=SEED,**Tips_P)\n\nprint('Model Training Iris After Tunning')\nprint(\"------------------------------------------------------\")\ntrained_model_I, train_scores, val_scores = Train_ML(model,'I')\nprint(\"------------------------------------------------------\")\n\nprint('\\n')\n\nprint('Model Training Tips After Tunning')\nprint(\"------------------------------------------------------\")\ntrained_model_T, train_scores, val_scores = Train_ML(model2,\"T\")","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:45:02.621970Z","iopub.execute_input":"2024-06-30T18:45:02.622752Z","iopub.status.idle":"2024-06-30T18:45:09.182956Z","shell.execute_reply.started":"2024-06-30T18:45:02.622715Z","shell.execute_reply":"2024-06-30T18:45:09.181859Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Model Training Iris After Tunning\n------------------------------------------------------\nFold 1: Train Accuracy = 1.0000, Validation Accuracy = 0.9333\nFold 2: Train Accuracy = 1.0000, Validation Accuracy = 1.0000\nFold 3: Train Accuracy = 1.0000, Validation Accuracy = 1.0000\nFold 4: Train Accuracy = 1.0000, Validation Accuracy = 1.0000\nFold 5: Train Accuracy = 1.0000, Validation Accuracy = 1.0000\nFold 6: Train Accuracy = 1.0000, Validation Accuracy = 1.0000\nFold 7: Train Accuracy = 1.0000, Validation Accuracy = 1.0000\nFold 8: Train Accuracy = 1.0000, Validation Accuracy = 1.0000\nFold 9: Train Accuracy = 1.0000, Validation Accuracy = 1.0000\nFold 10: Train Accuracy = 1.0000, Validation Accuracy = 1.0000\n\nMean Train Accuracy: 1.0000\nMean Validation Accuracy: 0.9933\n------------------------------------------------------\n\n\nModel Training Tips After Tunning\n------------------------------------------------------\nFold 1: Train Accuracy = 0.6575, Validation Accuracy = 0.7200\nFold 2: Train Accuracy = 0.6849, Validation Accuracy = 0.6400\nFold 3: Train Accuracy = 0.6895, Validation Accuracy = 0.6800\nFold 4: Train Accuracy = 0.6804, Validation Accuracy = 0.5600\nFold 5: Train Accuracy = 0.6818, Validation Accuracy = 0.6250\nFold 6: Train Accuracy = 0.6773, Validation Accuracy = 0.6667\nFold 7: Train Accuracy = 0.6818, Validation Accuracy = 0.5833\nFold 8: Train Accuracy = 0.6682, Validation Accuracy = 0.7083\nFold 9: Train Accuracy = 0.6818, Validation Accuracy = 0.7083\nFold 10: Train Accuracy = 0.6864, Validation Accuracy = 0.6250\n\nMean Train Accuracy: 0.6790\nMean Validation Accuracy: 0.6517\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now As we See , With Out Tunning The Params , We Are Getting the Accuracy of 0.93 and After tunning we get 100%. Which Is Very Good Score , also We Cross validate Our Model 10 Times Its Means Our Accurcy is Very good and Cross-Validate","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color: #00A4CCFF; font-family: Pacifico, cursive; color: #E6E6FA; font-size: 100%; text-align: left; border-radius: 5px; padding: 10px 30px; display: inline-block;\">LGBM Params Tunning</p>\n\n- **If you want to Study The LGBM Parameters :: https://lightgbm.readthedocs.io/en/stable/Parameters.html#objective-parameters**\n\n- **Parameter Tunning Guide :: https://lightgbm.readthedocs.io/en/stable/Parameters-Tuning.html**\n\n- **CatBoost Parameters :: https://catboost.ai/en/docs/references/training-parameters/**\n\n- **CatBoost Overfitting Detection :: https://catboost.ai/en/docs/references/training-parameters/overfitting-detection**","metadata":{}},{"cell_type":"code","source":"# # Ab ham Same Kam LGBM ke Liye Bhi Krien Gye aur Check Krien Gye es ke Score me Kia Improvements ati hain.\n\n# def objective(trial):\n    \n#     # Parameters to be optimized\n#     params = {\n#         'objective': 'multiclass',  # Yeh batata hai ke humari problem multiple classes pe based hai.\n#         'metric': 'multi_logloss',  # Evaluation metric jo multi-class log loss use karta hai.\n#         'verbosity': -1,  # Output ki details ko kam karta hai (silence mode).\n#         'boosting_type': 'gbdt',  # Gradient Boosting Decision Tree ko use karta hai.\n#         'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),  # L1 regularization parameter, Optuna se suggest hota hai.\n#         'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),  # L2 regularization parameter, Optuna se suggest hota hai.\n#         'num_leaves': trial.suggest_int('num_leaves', 2, 256),  # Decision tree ke leaves ki maximum count.\n#         'max_depth': trial.suggest_int('max_depth', 1, 50),  # Tree ki maximum depth.\n#         'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),  # Model ki learning rate, Optuna se suggest hota hai.\n#         'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),  # Har iteration mein use hone wale features ka fraction.\n#         'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),  # Bagging mein use hone wale data ka fraction.\n#         'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),  # Bagging ke intervals ki frequency.\n#         'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),  # Minimum samples jo ek leaf mein hone chahiye.\n#         'n_estimators': trial.suggest_int('n_estimators', 50, 1000),  # Boosting rounds ki total count.\n# #         'num_classes' : 3,\n# #         'device': 'gpu'\n#     # 'gpu_platform_id': 0,\n#     # 'gpu_device_id': 0\n#     }\n    \n#     val_scores = []\n#     n_splits = 10\n#     kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n\n#     for train_index, test_index in kf.split(X, y):\n#         X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n#         y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n#         model = LGBMClassifier(**params, random_state=SEED)\n#         model.fit(X_train, y_train)\n\n#         y_test_pred = model.predict(X_test)\n\n#         val_acc = accuracy_score(y_test, y_test_pred)\n#         val_scores.append(val_acc)\n\n#     mean_val_acc = np.mean(val_scores)\n#     return mean_val_acc\n\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=100)\n\n# best_params = study.best_params\n# print(\"Best parameters:\", best_params)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:44:14.397584Z","iopub.status.idle":"2024-06-30T18:44:14.398432Z","shell.execute_reply.started":"2024-06-30T18:44:14.398154Z","shell.execute_reply":"2024-06-30T18:44:14.398178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"# After 20 trails , ham ne Yeh Paramsters Hasil Kiye Hain Hain \n#  {'lambda_l1': 0.0026239141876874686, 'lambda_l2': 0.30759140476770286, 'num_leaves': 163, 'max_depth': 17,\n# 'learning_rate': 0.006675339914806045, 'feature_fraction': 0.6166615017267428, \n# 'bagging_fraction': 0.5213145800455292, 'bagging_freq': 4, 'min_child_samples': 35, 'n_estimators': 636}\"\"\"\n\n\"\"\"# Aur in Params pa Hamaray Model ka Score aa rha hy 1 , Jo ke ek Bht Amazing Change aya hy , Its means Optuna is Very \n# Good Option for Params Tunnings.\n\n# Chalien ek br Phr se apna Upper Wala Code Run Krty Hain , Aur Dekhaty Hain Kia Improvement ati hy \"\"\"\n\n# Idhr ham Tunned Params ko Use kRien Gye \nT_Params_LGBM =  {'lambda_l1': 0.0026239141876874686, 'lambda_l2': 0.30759140476770286, 'num_leaves': 163,\n                  'max_depth': 17, 'learning_rate': 0.006675339914806045, \n             'feature_fraction': 0.6166615017267428, 'bagging_fraction': 0.5213145800455292, 'bagging_freq': 4,\n                  'min_child_samples': 35, 'n_estimators': 636,'verbose':-1}\n\n# Ab Ham Apnay Model ko Check krty hain ke Kia , Params Ke Tunned krny se Kuch Faida Huwa ya nahi\nprint('Model Training Iris After Tunning')\nprint(\"------------------------------------------------------\")\nmodel = LGBMClassifier(**T_Params_LGBM,random_state=SEED)\n\navg_accuracy = Train_ML(model,\"I\")","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:46:58.133644Z","iopub.execute_input":"2024-06-30T18:46:58.134058Z","iopub.status.idle":"2024-06-30T18:46:59.042184Z","shell.execute_reply.started":"2024-06-30T18:46:58.134029Z","shell.execute_reply":"2024-06-30T18:46:59.041093Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Model Training Iris After Tunning\n------------------------------------------------------\nFold 1: Train Accuracy = 1.0000, Validation Accuracy = 1.0000\nFold 2: Train Accuracy = 1.0000, Validation Accuracy = 1.0000\nFold 3: Train Accuracy = 1.0000, Validation Accuracy = 1.0000\nFold 4: Train Accuracy = 1.0000, Validation Accuracy = 1.0000\nFold 5: Train Accuracy = 1.0000, Validation Accuracy = 1.0000\nFold 6: Train Accuracy = 1.0000, Validation Accuracy = 1.0000\nFold 7: Train Accuracy = 1.0000, Validation Accuracy = 1.0000\nFold 8: Train Accuracy = 1.0000, Validation Accuracy = 1.0000\nFold 9: Train Accuracy = 1.0000, Validation Accuracy = 1.0000\nFold 10: Train Accuracy = 1.0000, Validation Accuracy = 1.0000\n\nMean Train Accuracy: 1.0000\nMean Validation Accuracy: 1.0000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now As we See , With Out Tunning The Params Of LGBM , We Are Getting the Accuracy of 0.73 and After tunning we get 1.0. Which Is Very Good Score , also We Cross validate Our Model 10 Times Its Means Our Accurcy is Very good and Cross-Validate","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color: #00A4CCFF; font-family: Pacifico, cursive; color: #E6E6FA; font-size: 100%; text-align: left; border-radius: 5px; padding: 10px 30px; display: inline-block;\">Use Full Functions</p>","metadata":{}},{"cell_type":"code","source":"# '''             Scaling                         '''\n# # Function to Scale Data\n# def apply_scaling(data, columns, scaler_type):\n#     # Check the type of scaler and initialize the appropriate scaler object\n#     if scaler_type == 'S':\n#         scaler = StandardScaler()  # Initialize StandardScaler\n#     elif scaler_type == 'M':\n#         scaler = MinMaxScaler()  # Initialize MinMaxScaler\n#     elif scaler_type == 'R':\n#         scaler = RobustScaler()  # Initialize RobustScaler\n#     elif scaler_type == 'A':\n#         scaler = MaxAbsScaler()  # Initialize MaxAbsScaler\n#     elif scaler_type == 'Q':\n#         scaler = QuantileTransformer(output_distribution='normal')  # Initialize QuantileTransformer\n#     elif scaler_type == 'P':\n#         scaler = PowerTransformer()  # Initialize PowerTransformer\n#     else:\n#         raise ValueError(\"Invalid scaler type. Choose 'S' for StandardScaler, 'M' for MinMaxScaler, 'R' for RobustScaler, 'A' for MaxAbsScaler,'Q' for QuantileTransformer, or 'P' for PowerTransformer.\")\n\n#     # Create a copy of the input data to avoid modifying the original data\n#     scaled_data = data.copy()\n\n#     # Loop through each column to be scaled\n#     for col in columns:\n#         # Apply the scaler to the current column and update the data with the scaled values\n#         scaled_data[col] = scaler.fit_transform(scaled_data[[col]])\n\n#     # Return the scaled data\n#     return scaled_data\n\n# # Specify columns and scaler type\n# columns_to_scale = [col for col in diamond.columns if diamond[col].dtype == 'float']\n# scaler_type = 'S' \n\n# # Apply scaling data\n# diamond = apply_scaling(diamond, columns_to_scale, scaler_type)\n\n\n# '''             Encoding                         '''\n# # Function to Encode Data\n# def encode(data, columns, encoding_type='label'):\n#     try:\n#         # Initialize encoders\n#         label_encoder = LabelEncoder()\n#         one_hot_encoder = OneHotEncoder(sparse=False, drop='first')\n        \n#         # Idhr ham Data ki Copy bana Lyn Gye\n#         encoded_data = data.copy()\n        \n#         for col in columns:\n#             if encoding_type == 'label':\n#                 # Apply Label Encoding\n#                 encoded_data[col] = label_encoder.fit_transform(data[col])\n#             elif encoding_type == 'onehot':\n#                 # Idhr Ham Ohe Ko Apply krien gye Sth Sth New Transform Cols aur Dusray Cols ko Combine krien gye\n#                 one_hot_encoded = one_hot_encoder.fit_transform(data[[col]])\n#                 one_hot_encoded_df = pd.DataFrame(one_hot_encoded, columns=[f\"{col}_{category}\" for category in label_encoder.fit(data[col]).classes_[1:]])\n#                 encoded_data = pd.concat([encoded_data.drop(col, axis=1), one_hot_encoded_df], axis=1)\n#             else:\n#                 raise ValueError(\"Invalid encoding type. Choose 'label' for Label Encoding or 'onehot' for One-Hot Encoding.\")\n        \n#         return encoded_data\n\n#     except Exception as e:\n#         print(f\"An error occurred: {e}\")\n#         return None\n\n# # Call \n# columns_to_encode = add you Columns \n# encoding_type = Add Encoding Type Here\n\n# # Apply encoding to the data\n# diamond = encode(diamond, columns_to_encode, encoding_type)\n\n\n# '''             Train ML                         '''\n# def Train_ML(model,X,y):\n    \n#     skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED)\n#     train_scores = []\n#     val_scores = []\n\n#     for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n#         X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n#         y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n\n#         # Train the model\n#         model.fit(X_train, y_train)\n\n#         # Make predictions on training data\n#         y_train_pred = model.predict(X_train)\n#         train_accuracy = accuracy_score(y_train, y_train_pred)\n#         train_scores.append(train_accuracy)\n\n#         # Make predictions on validation data\n#         y_val_pred = model.predict(X_val)\n#         val_accuracy = accuracy_score(y_val, y_val_pred)\n#         val_scores.append(val_accuracy)\n\n#         # Print accuracy for this fold\n#         print(f\"Fold {fold}: Train Accuracy = {train_accuracy:.4f}, Validation Accuracy = {val_accuracy:.4f}\")\n\n#     # Calculate mean accuracy\n#     mean_train_accuracy = np.mean(train_scores)\n#     mean_val_accuracy = np.mean(val_scores)\n\n#     print(f\"\\nMean Train Accuracy: {mean_train_accuracy:.4f}\")\n#     print(f\"Mean Validation Accuracy: {mean_val_accuracy:.4f}\")\n\n#     return model, train_scores, val_scores\n\n# model = Here Add your Model With Params \n# trained_model_I, train_scores, val_scores = Train_ML(model,X,y)\n\n\n# # Ab ham Same Kam LGBM ke Liye Bhi Krien Gye aur Check Krien Gye es ke Score me Kia Improvements ati hain.\n\n# def objective(trial):\n    \n#     # Parameters to be optimized\n#     params = \"\"\"Add params According to you Model and Problem\"\"\"\n    \n#     val_scores = []\n    \n#     skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED) # Idhr Ap Koi aur bhi chose kr skty ho Apni Mrzi ka\n\n#     for train_index, test_index in kf.split(X, y):\n#         X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n#         y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n#         model = \"\"\"Model With Params\"\"\"\n#         model.fit(X_train, y_train)\n\n#         y_test_pred = model.predict(X_test)\n\n#         val_acc = accuracy_score(y_test, y_test_pred)\n#         val_scores.append(val_acc)\n\n#     mean_val_acc = np.mean(val_scores)\n#     return mean_val_acc\n\n# study = optuna.create_study(direction='maximize') # Ager to Accuracy Like Metrix hain to Maximun \n#                                                     # Ager Loss hy to minimize\n# study.optimize(objective, n_trials=100) # Trials ki Values Bhi Apni Choice pa Experiments kr skty hain\n\n# best_params = study.best_params\n# print(\"Best parameters:\", best_params)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T18:44:14.402459Z","iopub.status.idle":"2024-06-30T18:44:14.403391Z","shell.execute_reply.started":"2024-06-30T18:44:14.403108Z","shell.execute_reply":"2024-06-30T18:44:14.403134Z"},"trusted":true},"execution_count":null,"outputs":[]}]}